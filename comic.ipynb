{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json  # 2.0.9\n",
    "from openai import OpenAI # 1.61.1\n",
    "import gradio as gr # 5.7.1\n",
    "import time\n",
    "from generate_flux import generate_flux_schnell\n",
    "from generate_image import generate_image\n",
    "from extract_sections import extract_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a Comic artist called ComicCrafterAI. \"\n",
    "system_message += \"Your Goal is to generate a comic book style ultra short story based on user-provided prompts in not more than 50 words \"\n",
    "system_message += \"The Story is divided into four parts: Introduction: , Storyline: , Climax: ,  Moral: \"\n",
    "system_message += \"Generate the paragraph of each section and do not generate the content of each section in the form of panels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the API using OPENAI API interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma2b = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:5000/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our local chat streaming function\n",
    "### It utilises 'gemma-2-2b-it' instruction model which is best suited for the system or user defined chatbot interaction. It streams the response output and extracts the structured sections such as Introduction, Storyline, Climax and Moral through ' extract_sections.py ' module. The ' generate_image.py ' module takes each of the extracted content and provides the desired image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_local_streaming(message, image_function):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    response = gemma2b.chat.completions.create(\n",
    "        model=\"gemma-2-2b-it\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        stream=True  \n",
    "    )\n",
    "    reply = \"\"\" \"\"\"\n",
    "    yield reply, gr.update(value=None, visible=True, label=\"Processing...this may take upto 5 mins\")  \n",
    "    \n",
    "    for chunk in response:\n",
    "        text = chunk.choices[0].delta.content\n",
    "        if text:\n",
    "            reply += text\n",
    "            yield reply, gr.update(value=None, visible=True, label=\"Processing...this may take upto 5 mins\")  \n",
    "\n",
    "    sections = extract_sections(reply)\n",
    "    introduction = sections.get(\"introduction\", \"\")\n",
    "    storyline = sections.get(\"storyline\", \"\")\n",
    "    climax = sections.get(\"climax\", \"\")\n",
    "    moral = sections.get(\"moral\", \"\")\n",
    "    \n",
    "    image_generator = generate_flux_schnell if image_function == \"generate_flux_schnell\" else generate_image\n",
    "    \n",
    "    images = [\n",
    "        image_generator(introduction), \n",
    "        image_generator(storyline),\n",
    "        image_generator(climax),\n",
    "        image_generator(moral)\n",
    "    ]\n",
    "    \n",
    "    yield reply, gr.update(value=images, visible=True, label=\"Generated Comic Images\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We combine our functions and defined modules in a nice and user friendly Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_outputs = gr.Gallery(height=500, label=\"Generated Comic Images\", visible=True, columns=2) \n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "        image_function_selector = gr.Dropdown([\"generate_flux_schnell\", \"generate_image\"], label=\"Select Image Generation Function\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history, image_function):\n",
    "        history += [{\"role\": \"user\", \"content\": message}]\n",
    "        response_gen = chat_local_streaming(message, image_function)\n",
    "        reply = \"\"\n",
    "        images = []\n",
    "        for output in response_gen:\n",
    "            if isinstance(output, tuple):\n",
    "                reply, images = output\n",
    "            else:\n",
    "                reply = output\n",
    "            yield \"\", history + [{\"role\": \"assistant\", \"content\": reply}], images\n",
    "\n",
    "    entry.submit(\n",
    "        do_entry,\n",
    "        inputs=[entry, chatbot, image_function_selector],\n",
    "        outputs=[entry, chatbot, image_outputs]\n",
    "    )\n",
    "    clear.click(lambda: ([], gr.update(value=None, visible=False)), inputs=None, outputs=[chatbot, image_outputs], queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llmenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96893c8607dc97b003d35d006dea5dda501bef0f0b6219a24909f378e5a45af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
